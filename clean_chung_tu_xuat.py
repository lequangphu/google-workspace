# -*- coding: utf-8 -*-
"""Chung tu xuat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/115E2z_7iIHZnKgyRf5K1Nhg9x1a5D7FO
"""

import glob
import os
import re

import pandas as pd
from IPython.display import display

# --- 1. Imports and Global Definitions ---

# Define data_dir and file_pattern
data_dir = os.path.join(os.getcwd(), "data", "raw")
file_pattern = os.path.join(data_dir, "*CT.XUAT.csv")
# TARGET_FINAL_COLUMNS = ['Chứng từ', 'Ngày', 'Khách hàng', 'Mã Số', 'Tên', 'Số lượng', 'Giá bán', 'Thành tiền']

# --- 2. Helper Functions ---


def extract_year_month_from_filename(filepath):
    """
    Extracts the year and month numbers from a filename in the format 'YYYY_MM_CT.XUAT.csv'.
    """
    basename = os.path.basename(filepath)
    # Fixed regex: Use single backslash for \d and \. in raw string literal
    match = re.search(r"(\d{4})_(\d{1,2})_CT\.XUAT\.csv", basename)
    if match:
        return int(match.group(1)), int(match.group(2))
    return None, None  # Return None for both if not found


def extract_and_combine_headers(filepath):
    """
    Reads the first 5 rows of a CSV file using plain Python file reading,
    extracts headers from rows 4 and 5, combines them into a single,
    consistent list of header names, and ensures uniqueness.
    """
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            lines = [f.readline() for _ in range(5)]

        header_data = []
        for i in [3, 4]:  # Rows 4 and 5 are 0-indexed as 3 and 4
            if i < len(lines):
                header_data.append([item.strip() for item in lines[i].split(",")])
            else:
                header_data.append([])

        max_cols = max(len(row) for row in header_data)
        for row in header_data:
            while len(row) < max_cols:
                row.append("")

        df_preview = pd.DataFrame(header_data)
        header_row_4 = df_preview.iloc[0].fillna("").astype(str)
        header_row_5 = df_preview.iloc[1].fillna("").astype(str)

        combined_headers = []
        for h4, h5 in zip(header_row_4, header_row_5):
            if h4 and h5 and h4 != h5:
                combined_headers.append(f"{h4.strip()}_{h5.strip()}")
            elif h4:
                combined_headers.append(h4.strip())
            elif h5:
                combined_headers.append(h5.strip())
            else:
                combined_headers.append("Unnamed_Column")

        seen = {}
        unique_headers = []
        for header_name in combined_headers:
            original_name = header_name
            count = seen.get(original_name, 0)
            if count > 0:
                header_name = f"{original_name}_{count}"
            while header_name in unique_headers:
                count += 1
                header_name = f"{original_name}_{count}"
            unique_headers.append(header_name)
            seen[original_name] = count + 1

        return unique_headers

    except UnicodeDecodeError:
        try:
            with open(filepath, "r", encoding="latin1") as f:
                lines = [f.readline() for _ in range(5)]
            header_data = []
            for i in [3, 4]:
                if i < len(lines):
                    header_data.append([item.strip() for item in lines[i].split(",")])
                else:
                    header_data.append([])
            max_cols = max(len(row) for row in header_data)
            for row in header_data:
                while len(row) < max_cols:
                    row.append("")
            df_preview = pd.DataFrame(header_data)
            header_row_4 = df_preview.iloc[0].fillna("").astype(str)
            header_row_5 = df_preview.iloc[1].fillna("").astype(str)

            combined_headers = []
            for h4, h5 in zip(header_row_4, header_row_5):
                if h4 and h5 and h4 != h5:
                    combined_headers.append(f"{h4.strip()}_{h5.strip()}")
                elif h4:
                    combined_headers.append(h4.strip())
                elif h5:
                    combined_headers.append(h5.strip())
                else:
                    combined_headers.append("Unnamed_Column")

            seen = {}
            unique_headers = []
            for header_name in combined_headers:
                original_name = header_name
                count = seen.get(original_name, 0)
                if count > 0:
                    header_name = f"{original_name}_{count}"
                while header_name in unique_headers:
                    count += 1
                    header_name = f"{original_name}_{count}"
                unique_headers.append(header_name)
                seen[original_name] = count + 1

            return unique_headers

        except Exception:
            return [f"Encoding_Error_for_{os.path.basename(filepath)}"]

    except Exception:
        return [f"Error_Processing_Headers_for_{os.path.basename(filepath)}"]


# --- 3. Data Loading and Grouping Files by Header ---

# Find all CSV files
csv_files = glob.glob(file_pattern)

# Populate grouped_files_by_header
grouped_files_by_header = {}
for filepath in csv_files:
    combined_header = extract_and_combine_headers(filepath)
    header_tuple = tuple(combined_header)
    if header_tuple not in grouped_files_by_header:
        grouped_files_by_header[header_tuple] = []
    grouped_files_by_header[header_tuple].append(filepath)

# Initialize dictionaries for results
combined_dfs_by_group = {}

# --- 4. Main Data Processing Loop ---

for i, (header_tuple, filepaths) in enumerate(grouped_files_by_header.items()):
    dfs_for_current_group = []

    for filepath in filepaths:
        year_from_filename, month_from_filename = extract_year_month_from_filename(
            filepath
        )
        try:
            # Read CSV file with utf-8 encoding
            df = pd.read_csv(
                filepath,
                skiprows=5,
                header=None,
                names=list(header_tuple),
                encoding="utf-8",
                sep=",",
                engine="python",
            )

            # Add year and month from filename for later comparison/imputation
            df["year_from_filename"] = year_from_filename
            df["month_from_filename"] = month_from_filename

            # Date processing logic
            if "Ngày" in df.columns:
                # Attempt to parse 'Ngày' directly
                df["Ngày_parsed"] = pd.to_datetime(
                    df["Ngày"], format="%d/%m/%Y", errors="coerce"
                )

                if year_from_filename is not None and month_from_filename is not None:
                    day_only = pd.to_numeric(df["Ngày"], errors="coerce")
                    valid_day_mask = (
                        day_only.notna() & (day_only >= 1) & (day_only <= 31)
                    )

                    # Fill NaT in Ngày_parsed where Ngày was a valid day number but not a full date string
                    date_combined_strings = df.loc[
                        valid_day_mask & df["Ngày_parsed"].isna()
                    ].apply(
                        lambda row: f"{int(row['Ngày']):02d}/{month_from_filename:02d}/{year_from_filename}",
                        axis=1,
                    )
                    df.loc[date_combined_strings.index, "Ngày_parsed"] = pd.to_datetime(
                        date_combined_strings, format="%d/%m/%Y", errors="coerce"
                    )

                    # For any remaining NaT in Ngày_parsed, if filename info available,
                    # and original 'Ngày' is not a valid day, default to day 1 of the month/year from filename.
                    unparsed_mask = df["Ngày_parsed"].isna()
                    if unparsed_mask.any():
                        default_date_string = (
                            f"01/{month_from_filename:02d}/{year_from_filename}"
                        )
                        df.loc[unparsed_mask, "Ngày_parsed"] = pd.to_datetime(
                            default_date_string, format="%d/%m/%Y", errors="coerce"
                        )

                df["Ngày_Year"] = df["Ngày_parsed"].dt.year
                df["Ngày_Month"] = df["Ngày_parsed"].dt.month
                # Format 'Ngày' column to 'DD/MM/YYYY' or empty string if NaT
                df["Ngày"] = (
                    df["Ngày_parsed"].dt.strftime("%Y-%m-%d").fillna("")
                )  # Changed to YYYY-MM-DD
                df.drop(
                    columns=["Ngày_parsed"], inplace=True
                )  # Drop temporary parsing column
            else:
                df["Ngày_Year"] = pd.NA
                df["Ngày_Month"] = pd.NA

            dfs_for_current_group.append(df)
        except UnicodeDecodeError:
            # Retry with latin1 encoding if utf-8 fails
            try:
                df = pd.read_csv(
                    filepath,
                    skiprows=5,
                    header=None,
                    names=list(header_tuple),
                    encoding="latin1",
                    sep=",",
                    engine="python",
                )
                df["year_from_filename"] = year_from_filename
                df["month_from_filename"] = month_from_filename

                # Date processing logic for latin1 retry
                if "Ngày" in df.columns:
                    df["Ngày_parsed"] = pd.to_datetime(
                        df["Ngày"], format="%d/%m/%Y", errors="coerce"
                    )
                    if (
                        year_from_filename is not None
                        and month_from_filename is not None
                    ):
                        day_only = pd.to_numeric(df["Ngày"], errors="coerce")
                        valid_day_mask = (
                            day_only.notna() & (day_only >= 1) & (day_only <= 31)
                        )
                        date_combined_strings = df.loc[
                            valid_day_mask & df["Ngày_parsed"].isna()
                        ].apply(
                            lambda row: f"{int(row['Ngày']):02d}/{month_from_filename:02d}/{year_from_filename}",
                            axis=1,
                        )
                        df.loc[date_combined_strings.index, "Ngày_parsed"] = (
                            pd.to_datetime(
                                date_combined_strings,
                                format="%d/%m/%Y",
                                errors="coerce",
                            )
                        )

                        unparsed_mask = df["Ngày_parsed"].isna()
                        if unparsed_mask.any():
                            default_date_string = (
                                f"01/{month_from_filename:02d}/{year_from_filename}"
                            )
                            df.loc[unparsed_mask, "Ngày_parsed"] = pd.to_datetime(
                                default_date_string, format="%d/%m/%Y", errors="coerce"
                            )

                    df["Ngày_Year"] = df["Ngày_parsed"].dt.year
                    df["Ngày_Month"] = df["Ngày_parsed"].dt.month
                    df["Ngày"] = (
                        df["Ngày_parsed"].dt.strftime("%Y-%m-%d").fillna("")
                    )  # Changed to YYYY-MM-DD
                    df.drop(columns=["Ngày_parsed"], inplace=True)
                else:
                    df["Ngày_Year"] = pd.NA
                    df["Ngày_Month"] = pd.NA

                dfs_for_current_group.append(df)
            except Exception:
                # If latin1 also fails, skip the file
                pass
        except Exception:
            # For any other reading errors, skip the file
            pass

    if dfs_for_current_group:
        combined_df = pd.concat(dfs_for_current_group, ignore_index=True)

        # --- Column Renaming and Standardization ---

        # Conditional Renaming for Group 3 'Unnamed_Column' -> 'Mã Số' (specific header pattern)
        if (
            len(header_tuple) > 5
            and header_tuple[5] == "Unnamed_Column"
            and "Unnamed_Column" in combined_df.columns
        ):
            combined_df.rename(columns={"Unnamed_Column": "Mã Số"}, inplace=True)

        # Conditional Renaming for Group 2 'Mã số_MÃ SỐ' -> 'Mã Số' (specific header pattern)
        if "Mã số_MÃ SỐ" in combined_df.columns:
            combined_df.rename(columns={"Mã số_MÃ SỐ": "Mã Số"}, inplace=True)

        # Rename 'Chủng loại' -> 'Tên' for all groups (if exists)
        if "Chủng loại" in combined_df.columns:
            combined_df.rename(columns={"Chủng loại": "Tên"}, inplace=True)

        # Rename 'Chứng từ xuất_PXK' -> 'Chứng từ' for all groups (if exists)
        if "Chứng từ xuất_PXK" in combined_df.columns:
            combined_df.rename(columns={"Chứng từ xuất_PXK": "Chứng từ"}, inplace=True)

        # Calculate 'Số lượng' by summing 'Số lượng_Bán lẻ' and 'Bán sì'
        # Convert both to numeric, coercing errors to NaN, then fill NaN with 0 before summing
        so_luong_ban_le = pd.to_numeric(
            combined_df.get("Số lượng_Bán lẻ", pd.Series(index=combined_df.index)),
            errors="coerce",
        ).fillna(0)
        ban_si = pd.to_numeric(
            combined_df.get("Bán sì", pd.Series(index=combined_df.index)),
            errors="coerce",
        ).fillna(0)
        combined_df["Số lượng"] = so_luong_ban_le + ban_si

        # Drop rows where 'Số lượng' is zero or empty (NaN)
        combined_df = combined_df[
            combined_df["Số lượng"].notna() & (combined_df["Số lượng"] != 0)
        ]

        # Standardize 'Giá bán' and 'Thành tiền' if they exist, converting to numeric
        if "Giá bán" in combined_df.columns:
            combined_df["Giá bán"] = pd.to_numeric(
                combined_df["Giá bán"], errors="coerce"
            )
        if "Thành tiền" in combined_df.columns:
            combined_df["Thành tiền"] = pd.to_numeric(
                combined_df["Thành tiền"], errors="coerce"
            )

        combined_dfs_by_group[header_tuple] = combined_df

# Concatenate all processed DataFrames into a single final DataFrame
final_combined_df_refactored = pd.concat(
    combined_dfs_by_group.values(), ignore_index=True
)

# --- 5. Initial Verification of the Combined DataFrame ---

print("Shape of the initial combined DataFrame:", final_combined_df_refactored.shape)
print("\nHead of the initial combined DataFrame:")
display(final_combined_df_refactored.head())
print("\nInfo of the initial combined DataFrame:")
final_combined_df_refactored.info(verbose=True)

# --- 6. Date Consistency Check ---

# Convert to numeric type for 'Ngày_Year' and 'Ngày_Month' to handle potential mixed types and enable direct comparison
final_combined_df_refactored["Ngày_Year"] = pd.to_numeric(
    final_combined_df_refactored["Ngày_Year"], errors="coerce"
)
final_combined_df_refactored["Ngày_Month"] = pd.to_numeric(
    final_combined_df_refactored["Ngày_Month"], errors="coerce"
)

# Compare derived year with year from filename
year_mismatch = final_combined_df_refactored[
    (
        final_combined_df_refactored["Ngày_Year"]
        != final_combined_df_refactored["year_from_filename"]
    )
    & final_combined_df_refactored["Ngày_Year"].notna()
    & final_combined_df_refactored["year_from_filename"].notna()
]

# Compare derived month with month from filename
month_mismatch = final_combined_df_refactored[
    (
        final_combined_df_refactored["Ngày_Month"]
        != final_combined_df_refactored["month_from_filename"]
    )
    & final_combined_df_refactored["Ngày_Month"].notna()
    & final_combined_df_refactored["month_from_filename"].notna()
]

print(
    f"\nNumber of rows where 'Ngày_Year' does not match 'year_from_filename': {len(year_mismatch)}"
)
if not year_mismatch.empty:
    print("Sample of year mismatches:")
    display(year_mismatch[["Ngày", "Ngày_Year", "year_from_filename"]].head())

print(
    f"\nNumber of rows where 'Ngày_Month' does not match 'month_from_filename': {len(month_mismatch)}"
)
if not month_mismatch.empty:
    print("Sample of month mismatches:")
    display(month_mismatch[["Ngày", "Ngày_Month", "month_from_filename"]].head())

if year_mismatch.empty and month_mismatch.empty:
    print(
        "\nAll derived years and months from 'Ngày' column consistently match those extracted from the filename."
    )

# --- 7. Non-Null Value Analysis ---

# Calculate percentage of non-null values for all columns
non_null_percentage = (
    final_combined_df_refactored.count() / len(final_combined_df_refactored)
) * 100

print("\nPercentage of non-null values for each column:")
display(non_null_percentage.sort_values(ascending=False))

print("\nColumns having less than 90% non-null values:")
display(non_null_percentage[non_null_percentage < 90].index.tolist())

# --- 8. Final Column Cleanup and Renaming ---

# Define the list of auxiliary columns to drop
auxiliary_columns_to_drop = [
    "Số lượng_Bán lẻ",
    "PBH",
    "Tháng",
    "Unnamed_Column",
    "ĐVT",
    "Bán sì",
    "Unnamed_Column_1",
    "Unnamed_Column_2",
    "Unnamed_Column_3",
    "Unnamed_Column_4",
    "Unnamed_Column_5",
    "Unnamed_Column_6",
    "Unnamed_Column_7",
    "Unnamed_Column_8",
    "Ghi Chú",
    "Unnamed_Column_9",
    "Unnamed_Column_10",
    "164",
    "Tỉnh/TP",
    "Unnamed_Column_11",
    "Ngày_Year",
    "Ngày_Month",
    # Removed 'year_from_filename' and 'month_from_filename' from this list
]

# Drop the specified auxiliary columns if they exist in the DataFrame
final_combined_df_refactored = final_combined_df_refactored.drop(
    columns=[
        col
        for col in auxiliary_columns_to_drop
        if col in final_combined_df_refactored.columns
    ]
)

# User-specified renaming
rename_mapping = {
    "Chứng từ": "Mã chứng từ",
    "Ngày": "Ngày",
    "Khách hàng": "Tên khách hàng",
    "Mã Số": "Mã hàng",
    "Tên": "Tên hàng",
    "Giá bán": "Giá bán",
    "Thành tiền": "Thành tiền",
    "Số lượng": "Số lượng",
    "year_from_filename": "Năm",  # Renaming year_from_filename to Năm
    "month_from_filename": "Tháng",  # Renaming month_from_filename to Tháng
}

# Create a new mapping with only non-empty new names
filtered_rename_mapping = {
    old_name: new_name
    for old_name, new_name in rename_mapping.items()
    if new_name != ""
}

# Apply the renaming
final_combined_df_refactored.rename(columns=filtered_rename_mapping, inplace=True)

print("\n--- Final Cleaned and Renamed DataFrame --- ")
print("New DataFrame info:")
final_combined_df_refactored.info(verbose=True)
print("\nHead of the final cleaned and renamed DataFrame:")
display(final_combined_df_refactored.head())

# --- 9. Save Final DataFrame to CSV ---

# Create a temporary 'year_month_dt' column to represent the year and month as a datetime object
final_combined_df_refactored["year_month_dt"] = pd.to_datetime(
    final_combined_df_refactored["Năm"].astype(str)
    + "-"
    + final_combined_df_refactored["Tháng"].astype(str).str.zfill(2)
    + "-01",  # Assuming day 1 for consistent parsing
    errors="coerce",
)

# Drop rows where year_month_dt could not be parsed, as they are not valid for date range determination
valid_dates_df = final_combined_df_refactored.dropna(subset=["year_month_dt"])

if not valid_dates_df.empty:
    min_date = valid_dates_df["year_month_dt"].min()
    max_date = valid_dates_df["year_month_dt"].max()

    first_year = min_date.year
    first_month = min_date.month
    last_year = max_date.year
    last_month = max_date.month
else:
    # Fallback if no valid year/month combinations are found
    first_year = 0
    first_month = 0
    last_year = 0
    last_month = 0
    print(
        "Warning: Could not determine year/month range from 'Năm' and 'Tháng' columns for output filename. Using placeholders."
    )

# Format filename string using the extracted years and months
output_filename = (
    f"{first_year}_{first_month:02d}_{last_year}_{last_month:02d}_CT.XUAT_processed.csv"
)

# Determine the output directory
output_dir = os.path.join(os.getcwd(), "data", "final")
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# Construct the full output path
output_filepath = os.path.join(output_dir, output_filename)

# Save the DataFrame to CSV
final_combined_df_refactored.to_csv(output_filepath, index=False, encoding="utf-8")

print(f"\nFinal processed DataFrame saved to: {output_filepath}")

# Drop the temporary 'year_month_dt' column to clean up the DataFrame
final_combined_df_refactored.drop(
    columns=["year_month_dt"], inplace=True, errors="ignore"
)
