# -*- coding: utf-8 -*-
"""Xuat nhap ton (Inventory Export/Import/Stock) data processor.

Automatically generated by Colab.
Original file is located at
    https://colab.research.google.com/drive/1YrsWK_AB0M65-kaxSMz4Ki5sT5U81gpf
"""

import pandas as pd
import os
import csv
import re
import logging
from typing import Dict, List, Tuple, Optional
from pathlib import Path
from tqdm import tqdm

# ============================================================================
# CONFIGURATION
# ============================================================================

CONFIG = {
    "input_dir": os.path.join(os.getcwd(), "data", "raw"),
    "output_dir": os.path.join(os.getcwd(), "data", "final"),
    "file_pattern": r"(\d{4})_(\d{1,2})_XNT\.csv",
    "file_suffix": "XNT.csv",
    "min_non_null_percentage": 90,
    "skiprows": 5,
    "date_format": "%d-%m-%Y",
    "columns_to_convert": [
        "TỒN_ĐẦU_KỲ_Đ_GIÁ",
        "TỒN_CUỐI_KỲ_THÀNH_TIỀN",
        "TỒN_CUỐI_KỲ_DOANH_THU",
        "TỒN_CUỐI_KỲ_LÃI_GỘP",
    ],
    "column_rename_map": {
        "Mã_SP": "Mã hàng",
        "TÊN_HÀNG": "Tên hàng",
        "TỒN_ĐẦU_KỲ_S_LƯỢNG": "Số lượng đầu kỳ",
        "TỒN_ĐẦU_KỲ_Đ_GIÁ": "Đơn giá đầu kỳ",
        "TỒN_ĐẦU_KỲ_THÀNH_TIỀN": "Thành tiền đầu kỳ",
        "NHẬP_TRONG_KỲ_S_LƯỢNG": "Số lượng nhập trong kỳ",
        "NHẬP_TRONG_KỲ_Đ_GIÁ": "Đơn giá nhập trong kỳ",
        "NHẬP_TRONG_KỲ_THÀNH_TIỀN": "Thành tiền nhập trong kỳ",
        "XUẤT_TRONG_KỲ_SỐ_LƯỢNG": "Số lượng xuất trong kỳ",
        "XUẤT_TRONG_KỲ": "Xuất trong kỳ",
        "XUẤT_TRONG_KỲ_Đ_GIÁ": "Đơn giá xuất trong kỳ",
        "XUẤT_TRONG_KỲ_THÀNH_TIỀN": "Thành tiền xuất trong kỳ",
        "TỒN_CUỐI_KỲ_S_LƯỢNG": "Số lượng cuối kỳ",
        "TỒN_CUỐI_KỲ_Đ_GIÁ": "Đơn giá cuối kỳ",
        "TỒN_CUỐI_KỲ_THÀNH_TIỀN": "Thành tiền cuối kỳ",
        "TỒN_CUỐI_KỲ_DOANH_THU": "Doanh thu cuối kỳ",
        "TỒN_CUỐI_KỲ_LÃI_GỘP": "Lãi gộp cuối kỳ",
        "NGÀY": "Ngày",
    },
    "numeric_cols": [
        "Số lượng đầu kỳ",
        "Đơn giá đầu kỳ",
        "Thành tiền đầu kỳ",
        "Số lượng nhập trong kỳ",
        "Đơn giá nhập trong kỳ",
        "Thành tiền nhập trong kỳ",
        "Số lượng xuất trong kỳ",
        "Đơn giá xuất trong kỳ",
        "Thành tiền xuất trong kỳ",
        "Số lượng cuối kỳ",
        "Đơn giá cuối kỳ",
        "Thành tiền cuối kỳ",
        "Doanh thu cuối kỳ",
        "Lãi gộp cuối kỳ",
        "Biên lãi gộp",
    ],
}

# ============================================================================
# LOGGING SETUP
# ============================================================================

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================


def combine_headers(header_row_1: List[str], header_row_2: List[str]) -> List[str]:
    """
    Combines two header rows into a single list of clean, unique column names.

    Handles parent-child header relationships, empty cells, and duplicates.
    Preserves Vietnamese characters.

    Args:
        header_row_1: First header row (parent level)
        header_row_2: Second header row (child level)

    Returns:
        List of combined, cleaned column names
    """
    combined_names = []
    spanning_parent_header = ""
    name_counts = {}

    max_len = max(len(header_row_1), len(header_row_2))
    h1_padded = list(header_row_1) + [""] * (max_len - len(header_row_1))
    h2_padded = list(header_row_2) + [""] * (max_len - len(header_row_2))

    for i, (h1_val, h2_val) in enumerate(zip(h1_padded, h2_padded)):
        current_col_name = ""

        if h1_val and h1_val.strip() != "":
            spanning_parent_header = h1_val.strip()

        if h2_val and h2_val.strip() != "":
            if spanning_parent_header:
                current_col_name = f"{spanning_parent_header}_{h2_val.strip()}"
            else:
                current_col_name = h2_val.strip()
        elif spanning_parent_header:
            current_col_name = spanning_parent_header
        else:
            current_col_name = f"Unnamed_{i}"

        current_col_name = current_col_name.strip()
        current_col_name = current_col_name.replace(" ", "_")

        # Regex to keep alphanumeric, underscore, and Vietnamese characters
        current_col_name = re.sub(
            r"[^\wÁÀẢẠÃĂẰẮẲẶẶẬẤẦẨẪẬẬÉÈẺẼẸÊẾỀỂỄỆÍÌỈĨỊÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỴĐđ]",
            "_",
            current_col_name,
            flags=re.UNICODE,
        )
        current_col_name = re.sub(r"_{2,}", "_", current_col_name)
        current_col_name = current_col_name.strip("_")
        if not current_col_name:
            current_col_name = f"Unnamed_{i}"

        # Handle duplicates
        original_name = current_col_name
        count = name_counts.get(current_col_name, 0)
        temp_col_name = current_col_name
        while temp_col_name in combined_names:
            count += 1
            temp_col_name = f"{original_name}_{count}"
        name_counts[original_name] = count
        combined_names.append(temp_col_name)

    return combined_names


def extract_date_from_filename(filename: str) -> Optional[Tuple[str, str, str]]:
    """
    Extracts year, month, and formatted date from filename.

    Args:
        filename: Filename matching pattern YYYY_M_XNT.csv

    Returns:
        Tuple of (year, month_padded, date_string) or None if pattern not matched
    """
    match = re.match(CONFIG["file_pattern"], filename)
    if match:
        year = match.group(1)
        month = match.group(2).zfill(2)
        date_string = f"01-{month}-{year}"
        return year, month, date_string
    return None


# ============================================================================
# MAIN PROCESSING FUNCTIONS
# ============================================================================


def find_input_files() -> List[str]:
    """
    Finds all XNT.csv files in the input directory.

    Returns:
        List of file paths

    Raises:
        FileNotFoundError: If no XNT.csv files are found
    """
    input_dir = CONFIG["input_dir"]
    if not os.path.exists(input_dir):
        raise FileNotFoundError(f"Input directory not found: {input_dir}")

    all_files = os.listdir(input_dir)
    xnt_files = [
        os.path.join(input_dir, f)
        for f in all_files
        if f.endswith(CONFIG["file_suffix"]) and os.path.isfile(os.path.join(input_dir, f))
    ]

    if not xnt_files:
        raise FileNotFoundError(
            f"No files ending with '{CONFIG['file_suffix']}' found in '{input_dir}'"
        )

    logger.info(f"Found {len(xnt_files)} input files")
    return sorted(xnt_files)


def group_files_by_headers(file_paths: List[str]) -> Dict[Tuple, List[str]]:
    """
    Groups files by their header structure.

    Args:
        file_paths: List of file paths to process

    Returns:
        Dict mapping header tuples to lists of file paths
    """
    header_groups = {}
    errors = []

    for file_path in tqdm(file_paths, desc="Reading headers"):
        try:
            with open(file_path, "r", newline="", encoding="utf-8") as f:
                reader = csv.reader(f)
                rows = []
                for i, row in enumerate(reader):
                    rows.append(row)
                    if i == 3:
                        break

                if len(rows) > 3:
                    header_row_3 = list(rows[2])
                    header_row_4 = list(rows[3])
                    combined_column_names = combine_headers(header_row_3, header_row_4)
                    combined_header_key = tuple(combined_column_names)

                    if combined_header_key not in header_groups:
                        header_groups[combined_header_key] = []
                    header_groups[combined_header_key].append(file_path)
                else:
                    errors.append((file_path, "Insufficient header rows"))
        except Exception as e:
            errors.append((file_path, str(e)))

    if errors:
        logger.warning(f"Failed to read headers from {len(errors)} file(s)")
        for file_path, error in errors:
            logger.debug(f"  {os.path.basename(file_path)}: {error}")

    logger.info(f"Grouped files into {len(header_groups)} header groups")
    return header_groups


def load_and_process_group(
    combined_header_key: Tuple, file_paths: List[str]
) -> Optional[pd.DataFrame]:
    """
    Loads and consolidates dataframes for a group of files with matching headers.

    Args:
        combined_header_key: The header structure key for this group
        file_paths: List of file paths in this group

    Returns:
        Consolidated DataFrame or None if no valid data
    """
    group_dataframes = []
    errors = []

    for file_path in file_paths:
        try:
            filename = os.path.basename(file_path)
            date_info = extract_date_from_filename(filename)

            if not date_info:
                errors.append((filename, "Could not extract date from filename"))
                continue

            year, month, ngay_value = date_info

            df = pd.read_csv(
                file_path,
                skiprows=CONFIG["skiprows"],
                header=None,
                encoding="utf-8",
                engine="python",
                on_bad_lines="skip",
            )

            # Align columns with header
            if df.shape[1] > len(combined_header_key):
                df = df.iloc[:, : len(combined_header_key)]
            elif df.shape[1] < len(combined_header_key):
                for col_idx in range(df.shape[1], len(combined_header_key)):
                    df[f"Unnamed_missing_{col_idx}"] = pd.NA

            df.columns = combined_header_key
            df["Năm"] = year
            df["Tháng"] = month
            df["NGÀY"] = ngay_value
            df["NGÀY"] = pd.to_datetime(
                df["NGÀY"], errors="coerce", format=CONFIG["date_format"]
            )
            group_dataframes.append(df)

        except Exception as e:
            errors.append((os.path.basename(file_path), str(e)))

    if errors:
        logger.warning(f"Failed to load {len(errors)} file(s) from group")
        for filename, error in errors:
            logger.debug(f"  {filename}: {error}")

    if group_dataframes:
        return pd.concat(group_dataframes, ignore_index=True)
    return None


def consolidate_files(
    header_groups: Dict[Tuple, List[str]]
) -> Dict[str, pd.DataFrame]:
    """
    Consolidates all file groups into dataframes.

    Args:
        header_groups: Dict mapping headers to file paths

    Returns:
        Dict mapping group names to consolidated DataFrames
    """
    consolidated_dataframes = {}
    group_idx = 0

    for combined_header_key, file_paths in tqdm(
        header_groups.items(), desc="Processing groups"
    ):
        df = load_and_process_group(combined_header_key, file_paths)
        if df is not None:
            consolidated_dataframes[f"Group_{group_idx}"] = df
        group_idx += 1

    logger.info(f"Created {len(consolidated_dataframes)} consolidated group(s)")
    return consolidated_dataframes


def clean_data(consolidated_dataframes: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
    """
    Applies cleaning steps to each consolidated dataframe.

    Args:
        consolidated_dataframes: Dict of group name to DataFrame

    Returns:
        Dict of cleaned DataFrames with statistics
    """
    stats = {"rows_dropped_empty_code": 0, "columns_dropped": {}, "rows_dropped_empty_qty": 0}

    for group_name, df in list(consolidated_dataframes.items()):
        # Drop rows with empty product codes
        if "Mã_SP" in df.columns:
            df["Mã_SP"] = df["Mã_SP"].astype(str)
            rows_before = len(df)
            df_cleaned = df[df["Mã_SP"].str.strip() != ""]
            df_cleaned = df_cleaned[df_cleaned["Mã_SP"] != "nan"]
            rows_dropped = rows_before - len(df_cleaned)
            stats["rows_dropped_empty_code"] += rows_dropped
            consolidated_dataframes[group_name] = df_cleaned

        df = consolidated_dataframes[group_name]

        # Drop columns with low non-null coverage
        non_null_percentage = df.notna().sum() / len(df) * 100
        columns_to_drop = non_null_percentage[
            non_null_percentage < CONFIG["min_non_null_percentage"]
        ].index.tolist()

        # Preserve critical columns
        if "NGÀY" in columns_to_drop:
            columns_to_drop.remove("NGÀY")

        if columns_to_drop:
            stats["columns_dropped"][group_name] = columns_to_drop
            consolidated_dataframes[group_name] = df.drop(columns=columns_to_drop)

    logger.info(f"Dropped {stats['rows_dropped_empty_code']} rows with empty product codes")
    logger.info(f"Dropped columns from {len(stats['columns_dropped'])} group(s)")

    return consolidated_dataframes


def merge_and_refine(consolidated_dataframes: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """
    Merges all consolidated dataframes and applies final refinements.

    Args:
        consolidated_dataframes: Dict of DataFrames

    Returns:
        Final refined DataFrame
    """
    all_dfs = list(consolidated_dataframes.values())
    if not all_dfs:
        logger.warning("No data to merge")
        return pd.DataFrame()

    final_df = pd.concat(all_dfs, ignore_index=True)
    logger.info(f"Merged into final DataFrame with {len(final_df)} rows")

    # Convert numeric columns
    for col in CONFIG["columns_to_convert"]:
        if col in final_df.columns:
            final_df[col] = pd.to_numeric(
                final_df[col].astype(str).str.replace(",", "", regex=False),
                errors="coerce",
            )

    # Rename columns
    final_df = final_df.rename(columns=CONFIG["column_rename_map"])

    # Calculate profit margin
    if "Lãi gộp cuối kỳ" in final_df.columns and "Doanh thu cuối kỳ" in final_df.columns:
        revenue = final_df["Doanh thu cuối kỳ"]
        margin = final_df["Lãi gộp cuối kỳ"]
        valid_revenue = (pd.notna(revenue)) & (revenue != 0)
        final_df.loc[valid_revenue, "Biên lãi gộp"] = (
            margin[valid_revenue] / revenue[valid_revenue]
        )
        final_df.loc[~valid_revenue, "Biên lãi gộp"] = pd.NA

    # Drop redundant column
    if "Xuất trong kỳ" in final_df.columns:
        final_df = final_df.drop(columns=["Xuất trong kỳ"])
        logger.info("Dropped redundant 'Xuất trong kỳ' column")

    # Drop rows with no inventory
    so_luong_cols = [col for col in final_df.columns if col.startswith("Số lượng")]
    if so_luong_cols:
        for col in so_luong_cols:
            final_df[col] = pd.to_numeric(final_df[col], errors="coerce")

        mask_empty = final_df[so_luong_cols].isna().all(axis=1) | (
            final_df[so_luong_cols] == 0
        ).all(axis=1)
        rows_dropped = mask_empty.sum()
        final_df = final_df[~mask_empty]
        if rows_dropped > 0:
            logger.info(f"Dropped {rows_dropped} rows with no inventory")

    return final_df


def format_columns(final_df: pd.DataFrame) -> pd.DataFrame:
    """
    Formats and converts column data types.

    Args:
        final_df: DataFrame to format

    Returns:
        Formatted DataFrame
    """
    # Text columns
    if "Mã hàng" in final_df.columns:
        final_df["Mã hàng"] = final_df["Mã hàng"].astype(str).str.upper()
    if "Tên hàng" in final_df.columns:
        # Clean: strip leading/trailing spaces, collapse multiple spaces to single space
        final_df["Tên hàng"] = (
            final_df["Tên hàng"]
            .astype(str)
            .str.strip()
            .str.replace(r"\s+", " ", regex=True)
        )

    # Numeric columns
    for col in CONFIG["numeric_cols"]:
        if col in final_df.columns:
            final_df[col] = pd.to_numeric(final_df[col], errors="coerce")

    # Integer columns
    for col in ["Tháng", "Năm"]:
        if col in final_df.columns:
            final_df[col] = pd.to_numeric(final_df[col], errors="coerce").astype("Int64")

    # Sort
    if "Ngày" in final_df.columns and "Mã hàng" in final_df.columns:
        final_df["Ngày"] = pd.to_datetime(final_df["Ngày"], errors="coerce")
        final_df = final_df.sort_values(by=["Ngày", "Mã hàng"], na_position="last")

    return final_df


def save_results(final_df: pd.DataFrame) -> Optional[str]:
    """
    Saves the final DataFrame to CSV with appropriate filename.

    Args:
        final_df: Final DataFrame to save

    Returns:
        Path to output file or None if save failed
    """
    output_dir = CONFIG["output_dir"]
    os.makedirs(output_dir, exist_ok=True)

    if final_df.empty:
        logger.warning("Final DataFrame is empty. No output file generated.")
        return None

    if "Ngày" not in final_df.columns:
        logger.warning("'Ngày' column missing. No output file generated.")
        return None

    # Determine period from data
    final_df_valid = final_df.dropna(subset=["Ngày"])
    if final_df_valid.empty:
        output_filename = "unspecified_period_XNT_processed.csv"
    else:
        min_date = final_df_valid["Ngày"].min()
        max_date = final_df_valid["Ngày"].max()

        first_year = min_date.year
        first_month = str(min_date.month).zfill(2)
        last_year = max_date.year
        last_month = str(max_date.month).zfill(2)

        output_filename = f"{first_year}_{first_month}_{last_year}_{last_month}_XNT_processed.csv"

    output_filepath = os.path.join(output_dir, output_filename)
    final_df.to_csv(output_filepath, index=False, encoding="utf-8")
    logger.info(f"Saved output to: {output_filepath}")
    return output_filepath


def print_quality_report(final_df: pd.DataFrame) -> None:
    """
    Prints data quality report.

    Args:
        final_df: Final DataFrame
    """
    print("\n" + "=" * 70)
    print("DATA QUALITY REPORT")
    print("=" * 70)
    print(f"\nTotal rows: {len(final_df)}")
    print(f"Total columns: {len(final_df.columns)}")

    print("\nDataFrame Info:")
    final_df.info()

    if "Ngày" in final_df.columns:
        nat_count = final_df["Ngày"].isna().sum()
        if nat_count > 0:
            print(f"\nWarning: {nat_count} NaT values in 'Ngày' column")
        else:
            print("\nNo NaT values in 'Ngày' column")

    # Null value summary
    null_summary = final_df.isnull().sum()
    null_summary = null_summary[null_summary > 0]
    if not null_summary.empty:
        print("\nColumns with null values:")
        for col, count in null_summary.items():
            pct = (count / len(final_df)) * 100
            print(f"  {col}: {count} ({pct:.1f}%)")

    print("\n" + "=" * 70)


# ============================================================================
# MAIN EXECUTION
# ============================================================================


def main() -> None:
    """Main processing pipeline."""
    try:
        logger.info("Starting XNT data processing pipeline")

        # Load input files
        file_paths = find_input_files()

        # Group by headers
        header_groups = group_files_by_headers(file_paths)

        # Consolidate groups
        consolidated_dataframes = consolidate_files(header_groups)

        if not consolidated_dataframes:
            logger.error("No data could be processed")
            return

        # Clean data
        consolidated_dataframes = clean_data(consolidated_dataframes)

        # Merge and refine
        final_df = merge_and_refine(consolidated_dataframes)

        if final_df.empty:
            logger.error("Final DataFrame is empty after processing")
            return

        # Format columns
        final_df = format_columns(final_df)

        # Save results
        output_path = save_results(final_df)

        # Print quality report
        print_quality_report(final_df)

        logger.info("Processing completed successfully")

    except Exception as e:
        logger.error(f"Pipeline failed: {str(e)}", exc_info=True)
        raise


if __name__ == "__main__":
    main()
