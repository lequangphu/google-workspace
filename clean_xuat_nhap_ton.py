# -*- coding: utf-8 -*-
"""Xuat nhap ton.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YrsWK_AB0M65-kaxSMz4Ki5sT5U81gpf
"""

import pandas as pd
import os
import csv
import re

# 1. Define the corrected combine_headers function
def combine_headers(header_row_1, header_row_2):
    """
    Combines two header rows into a single list of clean, unique column names.
    Handles parent-child header relationships, empty cells, and duplicates.
    Preserves Vietnamese characters.
    """
    combined_names = []
    spanning_parent_header = ''
    name_counts = {}

    max_len = max(len(header_row_1), len(header_row_2))
    h1_padded = list(header_row_1) + [''] * (max_len - len(header_row_1))
    h2_padded = list(header_row_2) + [''] * (max_len - len(header_row_2))

    for i, (h1_val, h2_val) in enumerate(zip(h1_padded, h2_padded)):
        current_col_name = ''

        if h1_val and h1_val.strip() != '':
            spanning_parent_header = h1_val.strip()

        if h2_val and h2_val.strip() != '':
            if spanning_parent_header:
                current_col_name = f"{spanning_parent_header}_{h2_val.strip()}"
            else:
                current_col_name = h2_val.strip()
        elif spanning_parent_header:
            current_col_name = spanning_parent_header
        else:
            current_col_name = f"Unnamed_{i}"

        current_col_name = current_col_name.strip()
        current_col_name = current_col_name.replace(' ', '_')

        # Regex to keep alphanumeric, underscore, and specific Vietnamese characters
        # Anything else is replaced with an underscore
        current_col_name = re.sub(
            r'[^\wÁÀẢẠÃĂẰẮẲẶẶẬẤẦẨẪẬẬÉÈẺẼẸÊẾỀỂỄỆÍÌỈĨỊÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỴĐđ]',
            '_',
            current_col_name,
            flags=re.UNICODE
        )
        current_col_name = re.sub(r'_{2,}', '_', current_col_name)
        current_col_name = current_col_name.strip('_')
        if not current_col_name:
            current_col_name = f"Unnamed_{i}"

        original_name = current_col_name
        count = name_counts.get(current_col_name, 0)
        temp_col_name = current_col_name
        while temp_col_name in combined_names:
            count += 1
            temp_col_name = f"{original_name}_{count}"
        name_counts[original_name] = count
        combined_names.append(temp_col_name)

    return combined_names

# Directory path
directory_path = os.path.join(os.getcwd(), 'data', 'raw')

# Calculate output_dir once
output_dir = os.path.join(os.getcwd(), 'data', 'final')
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# 2. List all 'XNT.csv' files
all_files = os.listdir(directory_path)
xnt_csv_files = [f for f in all_files if f.endswith('XNT.csv') and os.path.isfile(os.path.join(directory_path, f))]

if not xnt_csv_files:
    raise FileNotFoundError(f"No files ending with 'XNT.csv' found in '{directory_path}'")

# 3. Group files based on their unique combined headers
header_groups = {}
for filename in xnt_csv_files:
    file_path = os.path.join(directory_path, filename)
    try:
        with open(file_path, 'r', newline='', encoding='utf-8') as f:
            reader = csv.reader(f)
            rows = []
            for i, row in enumerate(reader):
                rows.append(row)
                if i == 3:
                    break

            if len(rows) > 3:
                header_row_3 = list(rows[2])
                header_row_4 = list(rows[3])
                combined_column_names = combine_headers(header_row_3, header_row_4)
                combined_header_key = tuple(combined_column_names)

                if combined_header_key not in header_groups:
                    header_groups[combined_header_key] = []
                header_groups[combined_header_key].append(file_path)
            else:
                pass # Suppress warning for final script
    except Exception as e:
        pass # Suppress error for final script

# 4. Process and consolidate data for each group, adding a 'NGÀY' column
consolidated_dataframes = {}
group_idx = 0

for combined_header_key, file_paths in header_groups.items():
    group_dataframes = []
    for file_path in file_paths:
        try:
            filename = os.path.basename(file_path)
            match = re.match(r'(\d{4})_(\d{1,2})_XNT\.csv', filename)
            ngay_value = None
            if match:
                year = match.group(1)
                month = match.group(2).zfill(2)
                ngay_value = f'01-{month}-{year}'

            df = pd.read_csv(
                file_path,
                skiprows=5,
                header=None,
                encoding='utf-8',
                engine='python', # Use 'python' engine for better error handling
                on_bad_lines='skip' # Skip problematic lines
            )

            # Ensure DataFrame has the same number of columns as the header key
            if df.shape[1] > len(combined_header_key):
                df = df.iloc[:, :len(combined_header_key)]
            elif df.shape[1] < len(combined_header_key):
                # If fewer columns, add missing columns as NaN
                for col_idx in range(df.shape[1], len(combined_header_key)):
                    df[f'Unnamed_missing_{col_idx}'] = pd.NA

            df.columns = combined_header_key
            df['Năm'] = year
            df['Tháng'] = month
            df['NGÀY'] = ngay_value
            df['NGÀY'] = pd.to_datetime(df['NGÀY'], errors='coerce', format='%d-%m-%Y')
            group_dataframes.append(df)

        except Exception as e:
            pass # Suppress error for final script

    if group_dataframes:
        consolidated_df = pd.concat(group_dataframes, ignore_index=True)
        consolidated_dataframes[f'Group_{group_idx}'] = consolidated_df
    group_idx += 1

# Further cleaning steps applied to each consolidated DataFrame
for group_name, df in list(consolidated_dataframes.items()):

    # 5. Drop rows with empty 'Mã_SP' values
    if 'Mã_SP' in df.columns:
        df['Mã_SP'] = df['Mã_SP'].astype(str)
        df_cleaned = df[df['Mã_SP'].str.strip() != '']
        df_cleaned = df_cleaned[df_cleaned['Mã_SP'] != 'nan']
        consolidated_dataframes[group_name] = df_cleaned

    df = consolidated_dataframes[group_name]

    # 6. Drop columns identified as having less than 90% non-null values
    non_null_percentage = df.notna().sum() / len(df) * 100
    columns_to_drop = non_null_percentage[non_null_percentage < 90].index.tolist()

    # Ensure 'NGÀY' column is not dropped, as it's critical for further processing
    if 'NGÀY' in columns_to_drop:
        columns_to_drop.remove('NGÀY')

    if columns_to_drop:
        consolidated_dataframes[group_name] = df.drop(columns=columns_to_drop)

# 8. Combine all consolidated dataframes into a single final DataFrame
all_consolidated_dfs = []
for group_name, df in consolidated_dataframes.items():
    all_consolidated_dfs.append(df)

if all_consolidated_dfs:
    final_df = pd.concat(all_consolidated_dfs, ignore_index=True)
else:
    final_df = pd.DataFrame()

# Numeric conversion
columns_to_convert = [
    'TỒN_ĐẦU_KỲ_Đ_GIÁ',
    'TỒN_CUỐI_KỲ_THÀNH_TIỀN',
    'TỒN_CUỐI_KỲ_DOANH_THU',
    'TỒN_CUỐI_KỲ_LÃI_GỘP'
]

for col in columns_to_convert:
    if col in final_df.columns:
        # The previous code incorrectly removed the decimal point ('.') when it was present in the raw data.
        # This line now removes only commas (assuming they are thousands separators) and then converts to numeric,
        # allowing dot decimals to be parsed correctly.
        final_df[col] = pd.to_numeric(final_df[col].astype(str).str.replace(',', '', regex=False), errors='coerce')

# Rename columns
column_rename_map = {
    'Mã_SP': 'Mã hàng',
    'TÊN_HÀNG': 'Tên hàng',
    'TỒN_ĐẦU_KỲ_S_LƯỢNG': 'Số lượng đầu kỳ',
    'TỒN_ĐẦU_KỲ_Đ_GIÁ': 'Đơn giá đầu kỳ',
    'TỒN_ĐẦU_KỲ_THÀNH_TIỀN': 'Thành tiền đầu kỳ',
    'NHẬP_TRONG_KỲ_S_LƯỢNG': 'Số lượng nhập trong kỳ',
    'NHẬP_TRONG_KỲ_Đ_GIÁ': 'Đơn giá nhập trong kỳ',
    'NHẬP_TRONG_KỲ_THÀNH_TIỀN': 'Thành tiền nhập trong kỳ',
    'XUẤT_TRONG_KỲ_SỐ_LƯỢNG': 'Số lượng xuất trong kỳ',
    'XUẤT_TRONG_KỲ': 'Xuất trong kỳ',
    'XUẤT_TRONG_KỲ_Đ_GIÁ': 'Đơn giá xuất trong kỳ',
    'XUẤT_TRONG_KỲ_THÀNH_TIỀN': 'Thành tiền xuất trong kỳ',
    'TỒN_CUỐI_KỲ_S_LƯỢNG': 'Số lượng cuối kỳ',
    'TỒN_CUỐI_KỲ_Đ_GIÁ': 'Đơn giá cuối kỳ',
    'TỒN_CUỐI_KỲ_THÀNH_TIỀN': 'Thành tiền cuối kỳ',
    'TỒN_CUỐI_KỲ_DOANH_THU': 'Doanh thu cuối kỳ',
    'TỒN_CUỐI_KỲ_LÃI_GỘP': 'Lãi gộp cuối kỳ',
    'NGÀY': 'Ngày'
}
final_df = final_df.rename(columns=column_rename_map)

# Add "Biên lãi gộp" column = "Lãi gộp cuối kỳ" / "Doanh thu cuối kỳ"
# Vectorized operation is much faster than apply() for large datasets
if 'Lãi gộp cuối kỳ' in final_df.columns and 'Doanh thu cuối kỳ' in final_df.columns:
    revenue = final_df['Doanh thu cuối kỳ']
    margin = final_df['Lãi gộp cuối kỳ']
    # Only divide where revenue is non-null and non-zero; otherwise NA
    valid_revenue = (pd.notna(revenue)) & (revenue != 0)
    final_df.loc[valid_revenue, 'Biên lãi gộp'] = margin[valid_revenue] / revenue[valid_revenue]
    final_df.loc[~valid_revenue, 'Biên lãi gộp'] = pd.NA

# Drop 'Xuất trong kỳ' column if it exists (redundant with detailed breakdown)
if 'Xuất trong kỳ' in final_df.columns:
    final_df = final_df.drop(columns=['Xuất trong kỳ'])
    print("Dropped redundant 'Xuất trong kỳ' column")

# --- Drop rows where all "Số lượng*" columns are empty or zero ---
so_luong_cols = [col for col in final_df.columns if col.startswith('Số lượng')]
if so_luong_cols:
    # Convert to numeric, handling NaN and invalid values
    for col in so_luong_cols:
        final_df[col] = pd.to_numeric(final_df[col], errors='coerce')
    
    # Create mask for rows where all Số lượng columns are NaN or 0
    mask_empty_rows = final_df[so_luong_cols].isna().all(axis=1) | (final_df[so_luong_cols] == 0).all(axis=1)
    rows_dropped = mask_empty_rows.sum()
    final_df = final_df[~mask_empty_rows]
    if rows_dropped > 0:
        print(f"Dropped {rows_dropped} rows with no inventory (all Số lượng columns empty or zero)")

# --- Format columns before saving ---
# Ensure text columns are strings, numeric columns are numeric
if "Mã hàng" in final_df.columns:
    final_df["Mã hàng"] = final_df["Mã hàng"].astype(str).str.upper()
if "Tên hàng" in final_df.columns:
    final_df["Tên hàng"] = final_df["Tên hàng"].astype(str)

# Numeric columns (quantities and prices)
numeric_cols = [
    "Số lượng đầu kỳ", "Đơn giá đầu kỳ", "Thành tiền đầu kỳ",
    "Số lượng nhập trong kỳ", "Đơn giá nhập trong kỳ", "Thành tiền nhập trong kỳ",
    "Số lượng xuất trong kỳ", "Đơn giá xuất trong kỳ", "Thành tiền xuất trong kỳ",
    "Số lượng cuối kỳ", "Đơn giá cuối kỳ", "Thành tiền cuối kỳ",
    "Doanh thu cuối kỳ", "Lãi gộp cuối kỳ", "Biên lãi gộp"
]
for col in numeric_cols:
    if col in final_df.columns:
        final_df[col] = pd.to_numeric(final_df[col], errors="coerce")

# Integer columns
for col in ["Tháng", "Năm"]:
    if col in final_df.columns:
        final_df[col] = pd.to_numeric(final_df[col], errors="coerce").astype("Int64")

# --- Sort by Ngày and Mã hàng ---
if 'Ngày' in final_df.columns and 'Mã hàng' in final_df.columns:
    final_df['Ngày_dt'] = pd.to_datetime(final_df['Ngày'], errors='coerce')
    final_df = final_df.sort_values(by=['Ngày_dt', 'Mã hàng'], na_position='last')
    final_df = final_df.drop(columns=['Ngày_dt'])

# 9. Determine the period for the output filename and save the final DataFrame
if not final_df.empty and 'Ngày' in final_df.columns:
    # Convert 'Ngày' column to datetime, handling potential errors
    final_df['Ngày'] = pd.to_datetime(final_df['Ngày'], errors='coerce')

    # Drop rows where 'Ngày' could not be parsed
    final_df_valid_dates = final_df.dropna(subset=['Ngày'])

    if not final_df_valid_dates.empty:
        min_date = final_df_valid_dates['Ngày'].min()
        max_date = final_df_valid_dates['Ngày'].max()

        first_year = min_date.year
        first_month = min_date.month
        last_year = max_date.year
        last_month = max_date.month

        first_month_str = str(first_month).zfill(2)
        last_month_str = str(last_month).zfill(2)

        output_filename = f"{first_year}_{first_month_str}_{last_year}_{last_month_str}_XNT_processed.csv"

        output_filepath = os.path.join(output_dir, output_filename)

        final_df.to_csv(output_filepath, index=False, encoding='utf-8')
        print(f"Final processed data saved to: {output_filepath}")
    else:
        output_filename = "unspecified_period_XNT_processed.csv"
        output_filepath = os.path.join(output_dir, output_filename)
        final_df.to_csv(output_filepath, index=False, encoding='utf-8')
        print(f"Final processed data saved to: {output_filepath}")
else:
    print("Final DataFrame is empty or 'Ngày' column is missing. No output file generated.")

# Final DataFrame Info and date checks (minimal for validation)
print("\nFinal DataFrame Info:")
final_df.info()

if 'Ngày' in final_df.columns:
    nat_count = final_df['Ngày'].isna().sum()
    if nat_count > 0:
        print(f"\nWarning: There are {nat_count} NaT values remaining in the 'Ngày' column.")
    else:
        print("\nNo NaT values found in the 'Ngày' column.")