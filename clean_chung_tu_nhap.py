# -*- coding: utf-8 -*-
"""Chung tu nhap.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gtKpIzp6gRXDPzuVwe3eTUa_cZFKZNfe
"""

import csv
import glob
import os
from collections import defaultdict
from datetime import datetime

import pandas as pd


def combine_headers(header_row1, header_row2):
    final_combined_headers = [
        f"{header_row1[0].strip()}_{header_row2[0].strip()}",  # Chứng từ nhập_PXH
        f"{header_row1[0].strip()}_{header_row2[1].strip()}",  # Chứng từ nhập_PNK
        f"{header_row1[0].strip()}_{header_row2[2].strip()}",  # Chứng từ nhập_Ngày
        header_row2[3].strip(),  # Nhà CC
        header_row1[4].strip(),  # Người mua
        header_row1[5].strip(),  # Mã HH
        header_row1[6].strip(),  # Chủng loại
        header_row1[7].strip(),  # ĐVT
        f"{header_row1[8].strip()}_{header_row2[8].strip()}",  # Số lượng_Kho 1
        f"{header_row1[8].strip()}_{header_row2[9].strip()}",  # Số lượng_Kho 2
        f"{header_row1[8].strip()}_{header_row2[10].strip()}",  # Số lượng_Kho 3
        f"{header_row1[8].strip()}_{header_row2[14].strip()}",  # Số lượng_Asc
        f"{header_row1[8].strip()}_{header_row2[15].strip()}",  # Số lượng_Đào Khánh
        header_row1[22].strip(),  # Ghi chú
        header_row1[23].strip().replace("\n", " "),  # Đơn giá nhập
        header_row1[24].strip(),  # Thành tiền
        header_row1[25].strip(),  # GHI CHÚ
        f"{header_row1[26].strip()}_{header_row2[26].strip()}",  # Thời hạn bảo hành_Thời gian
        f"{header_row1[26].strip()}_{header_row2[27].strip()}",  # Thời hạn bảo hành_Hết hạn
        f"{header_row1[26].strip()}_{header_row2[28].strip()}",  # Thời hạn bảo hành_Gia hạn
    ]
    original_indices = [
        0,  # Chứng từ nhập_PXH (from header_row1[0])
        1,  # Chứng từ nhập_PNK (from header_row2[1])
        2,  # Chứng từ nhập_Ngày (from header_row2[2])
        3,  # Nhà CC (from header_row2[3])
        4,  # Người mua (from header_row1[4])
        5,  # Mã HH (from header_row1[5])
        6,  # Chủng loại (from header_row1[6])
        7,  # ĐVT (from header_row1[7])
        8,  # Số lượng_Kho 1 (from header_row2[8])
        9,  # Số lượng_Kho 2 (from header_row2[9])
        10,  # Số lượng_Kho 3 (from header_row2[10])
        14,  # Số lượng_Asc (from header_row2[14])
        15,  # Số lượng_Đào Khánh (from header_row2[15])
        22,  # Ghi chú (from header_row1[22])
        23,  # Đơn giá nhập (from header_row1[23])
        24,  # Thành tiền (from header_row1[24])
        25,  # GHI CHÚ (from header_row1[25])
        26,  # Thời hạn bảo hành_Thời gian (from header_row2[26])
        27,  # Thời hạn bảo hành_Hết hạn (from header_row2[27])
        28,  # Thời hạn bảo hành_Gia hạn (from header_row2[28])
    ]
    return final_combined_headers, original_indices


def parse_date_robustly(row):
    date_str = row["Ngày"]
    source_month = row["_source_file_month"]
    source_year = row["_source_file_year"]

    if pd.isna(date_str) or not isinstance(date_str, str):
        return pd.NaT

    date_str = str(date_str).strip()

    # --- Stage 1: Try unambiguous 4-digit year formats ---
    for fmt in [
        "%Y/%m/%d",  # e.g., 2021/04/14
        "%Y-%m-%d",  # e.g., 2021-02-18
    ]:
        try:
            return pd.to_datetime(date_str, format=fmt, errors="raise")
        except ValueError:
            pass

    # --- Stage 2: Handle ambiguous 4-digit year formats (DD/MM/YYYY vs MM/DD/YYYY) ---
    parsed_dmy = pd.NaT
    try:
        parsed_dmy = pd.to_datetime(date_str, format="%d/%m/%Y", errors="raise")
    except ValueError:
        pass

    parsed_mdy = pd.NaT
    try:
        parsed_mdy = pd.to_datetime(date_str, format="%m/%d/%Y", errors="raise")
    except ValueError:
        pass

    if pd.notna(parsed_dmy) and pd.notna(parsed_mdy):
        if parsed_mdy.month == source_month and parsed_dmy.month != source_month:
            return parsed_mdy
        elif parsed_dmy.month == source_month and parsed_mdy.month != source_month:
            return parsed_dmy
        else:
            return parsed_dmy
    elif pd.notna(parsed_dmy):
        return parsed_dmy
    elif pd.notna(parsed_mdy):
        return parsed_mdy

    # --- Stage 3: Handle 2-digit year ambiguities, guided by _source_file_month ---
    dmy_parsed = pd.NaT
    try:
        dmy_parsed = pd.to_datetime(date_str, format="%d/%m/%y", errors="raise")
    except ValueError:
        pass

    mdy_parsed = pd.NaT
    try:
        mdy_parsed = pd.to_datetime(date_str, format="%m/%d/%y", errors="raise")
    except ValueError:
        pass

    ymd_parsed = pd.NaT
    try:
        ymd_parsed = pd.to_datetime(date_str, format="%y/%m/%d", errors="raise")
    except ValueError:
        pass

    potential_dates = []
    if pd.notna(dmy_parsed):
        potential_dates.append(dmy_parsed)
    if pd.notna(mdy_parsed):
        potential_dates.append(mdy_parsed)
    if pd.notna(ymd_parsed):
        potential_dates.append(ymd_parsed)

    if len(potential_dates) == 1:
        return potential_dates[0]
    elif len(potential_dates) > 1:
        if pd.notna(dmy_parsed) and dmy_parsed.month == source_month:
            return dmy_parsed
        if pd.notna(mdy_parsed) and mdy_parsed.month == source_month:
            return mdy_parsed
        if pd.notna(ymd_parsed) and ymd_parsed.month == source_month:
            return ymd_parsed
        if pd.notna(dmy_parsed):
            return dmy_parsed
        elif pd.notna(mdy_parsed):
            return mdy_parsed
        elif pd.notna(ymd_parsed):
            return ymd_parsed

    # --- Stage 4: Other strict formats (e.g., with dashes) ---
    for fmt in [
        "%d-%m-%Y",  # e.g., 18-02-2021
        "%m-%d-%Y",  # e.g., 02-18-2021
        "%d-%m-%y",  # e.g., 18-02-21
        "%m-%d-%y",  # e.g., 02-18-21
    ]:
        try:
            return pd.to_datetime(date_str, format=fmt, errors="raise")
        except ValueError:
            pass

    # --- Stage 5: Fallback to general parsing ---
    try:
        return pd.to_datetime(date_str, dayfirst=True, errors="raise")
    except ValueError:
        pass

    # --- Stage 6: Final fallback for truly unparsable strings using source_year/month ---
    try:
        if pd.notna(source_month) and pd.notna(source_year) and 1 <= source_month <= 12:
            return pd.to_datetime(f"{source_year}-{source_month}-01")
    except Exception:
        pass

    return pd.NaT


def is_float_check(value):
    try:
        float(value)
        return True
    except (ValueError, TypeError):
        return False


# --- 1. Initial Data Loading, Processing & Combination ---
directory_path = os.path.join(os.getcwd(), "data", "raw")
file_pattern = "*CT.NHAP.csv"
full_pattern = os.path.join(directory_path, file_pattern)
matching_files = glob.glob(full_pattern)

if len(matching_files) == 0:
    print("No CSV files found for processing.")
else:
    print(f"Processing {len(matching_files)} CSV file(s)...")

file_headers_map = {}
for file_path in matching_files:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            reader = csv.reader(f)
            rows = list(reader)

        if len(rows) > 4:
            header_row_main = rows[3]
            header_row_sub = rows[4]
            combined_header, original_indices = combine_headers(
                header_row_main, header_row_sub
            )
            file_headers_map[file_path] = (combined_header, original_indices)
        else:
            print(
                f"Warning: File {file_path} has fewer than 5 rows, skipping header extraction."
            )
    except Exception as e:
        print(f"Error processing file {file_path} for headers: {e}")

grouped_files = defaultdict(list)
for file_path, (headers, indices) in file_headers_map.items():
    grouped_files[tuple(headers)].append((file_path, indices))

merged_dataframes = {}
for i, (headers_tuple, files_and_indices_list) in enumerate(grouped_files.items()):
    common_headers = list(headers_tuple)

    group_dfs = []
    for file_path, original_indices in files_and_indices_list:
        try:
            filename = file_path.split("/")[-1]
            parts = filename.split("_")
            source_year = int(parts[0])
            source_month = int(parts[1])

            with open(file_path, "r", encoding="utf-8") as f:
                reader = csv.reader(f)
                all_rows = list(reader)

            if len(all_rows) < 5:
                print(
                    f"Warning: File {file_path} has fewer than 5 rows, skipping data extraction."
                )
                continue

            data_rows = all_rows[5:]
            # Create a DataFrame using only the relevant columns based on original_indices
            # Ensure data_rows are long enough before trying to access columns
            processed_data = []
            for row in data_rows:
                if len(row) > max(original_indices) if original_indices else 0:
                    processed_data.append([row[idx] for idx in original_indices])
                else:
                    # Handle rows that are too short by filling with pd.NA for missing columns
                    new_row = []
                    for idx in original_indices:
                        if idx < len(row):
                            new_row.append(row[idx])
                        else:
                            new_row.append(pd.NA)
                    processed_data.append(new_row)

            df = pd.DataFrame(processed_data)
            df = df.replace("", pd.NA)

            df.columns = common_headers
            df["_source_file_month"] = source_month
            df["_source_file_year"] = source_year
            group_dfs.append(df)
        except Exception as e:
            print(f"Error loading or processing file {file_path} for data: {e}")

    if group_dfs:
         merged_df = pd.concat(group_dfs, ignore_index=True)
         merged_dataframes[f"Group_{i + 1}"] = merged_df

columns_to_drop_group1 = [
    "Chứng từ nhập_PXH",
    "Người mua",
    "ĐVT",
    "Số lượng_Kho 2",
    "Số lượng_Kho 3",
    "Số lượng_Asc",
    "Số lượng_Đào Khánh",
    "Ghi chú",
    "GHI CHÚ",
    "Thời hạn bảo hành_Thời gian",
    "Thời hạn bảo hành_Hết hạn",
    "Thời hạn bảo hành_Gia hạn",
]
columns_to_drop_group2 = [
    "Chứng từ nhập_PXH",
    "Người mua",
    "ĐVT",
    "Số lượng_Kho 2",
    "Số lượng_Kho 3",
    "Số lượng_Asc",
    "Số lượng_Đào Khánh",
    "Ghi chú",
    "Nhà SX",
    "Thời hạn bảo hành_Thời gian",
    "Thời hạn bảo hành_Hết hạn",
    "Thời hạn bảo hành_Gia hạn",
]
rename_mapping = {
    "Chứng từ nhập_PNK": "Mã chứng từ",
    "Chứng từ nhập_Ngày": "Ngày",
    "Nhà CC": "Tên nhà cung cấp",
    "Mã HH": "Mã hàng",
    "Chủng loại": "Tên hàng",
    "Số lượng_Kho 1": "Số lượng",
    "Đơn giá  nhập": "Đơn giá",
    "Thành tiền": "Thành tiền",
}

if "Group_1" in merged_dataframes:
    merged_dataframes["Group_1"] = merged_dataframes["Group_1"].dropna(
        subset=["Số lượng_Kho 1"]
    )
    merged_dataframes["Group_1"] = merged_dataframes["Group_1"].drop(
        columns=columns_to_drop_group1, errors="ignore"
    )
    merged_dataframes["Group_1"] = merged_dataframes["Group_1"].rename(
        columns=rename_mapping
    )

if "Group_2" in merged_dataframes:
    merged_dataframes["Group_2"] = merged_dataframes["Group_2"].dropna(
        subset=["Số lượng_Kho 1"]
    )
    merged_dataframes["Group_2"] = merged_dataframes["Group_2"].drop(
        columns=columns_to_drop_group2, errors="ignore"
    )
    merged_dataframes["Group_2"] = merged_dataframes["Group_2"].rename(
        columns=rename_mapping
    )

final_combined_df = pd.concat(
     [df for df in merged_dataframes.values()], ignore_index=True
 )

# --- 2. Comprehensive Date Cleaning for 'Ngày' Column ---
# Create date_analysis_df to separate float and string dates
date_analysis_df = final_combined_df[
    ["Ngày", "_source_file_month", "_source_file_year"]
].copy()

float_mask = date_analysis_df["Ngày"].apply(is_float_check)
float_dates_df = date_analysis_df[float_mask].copy()
string_dates_df = date_analysis_df[~float_mask].copy()

# Apply robust parsing function to string_dates_df
string_dates_df["Parsed Ngày"] = string_dates_df.apply(parse_date_robustly, axis=1)

# Convert Excel Serial Dates in float_dates_df
float_dates_df["Ngày"] = pd.to_numeric(float_dates_df["Ngày"], errors="coerce")
float_dates_df["Parsed Ngày"] = pd.to_datetime(
    float_dates_df["Ngày"], unit="D", origin="1899-12-30", errors="coerce"
)

# Initialize a new Series for the final 'Ngày' column in final_combined_df
final_combined_df_processed_dates = pd.Series(
    pd.NaT, index=final_combined_df.index, dtype="datetime64[ns]"
)

# Populate with Parsed Ngày from string_dates_df
final_combined_df_processed_dates.update(string_dates_df["Parsed Ngày"])

# Populate with Parsed Ngày from float_dates_df
final_combined_df_processed_dates.update(float_dates_df["Parsed Ngày"])

# Assign this processed Series back to final_combined_df['Ngày']
final_combined_df["Ngày"] = final_combined_df_processed_dates

# Final type coercion to ensure consistency
final_combined_df["Ngày"] = pd.to_datetime(final_combined_df["Ngày"], errors="coerce")

# --- 3. Handle Conflicting Entries with Backward Fill ---
# Re-identify mismatches in the current state of final_combined_df for bfill consideration
# (Using temporary columns to verify against original _source_file_year/_month)
final_combined_df["Verified_Ngày_temp_bfill"] = pd.to_datetime(
    final_combined_df["Ngày"], errors="coerce"
)
mismatch_mask_bfill = final_combined_df["Verified_Ngày_temp_bfill"].notna() & (
    (
        final_combined_df["Verified_Ngày_temp_bfill"].dt.year
        != final_combined_df["_source_file_year"]
    )
    | (
        final_combined_df["Verified_Ngày_temp_bfill"].dt.month
        != final_combined_df["_source_file_month"]
    )
)
mismatched_rows_bfill = final_combined_df[mismatch_mask_bfill]

if not mismatched_rows_bfill.empty:
    mismatch_indices_bfill = mismatched_rows_bfill.index
    # For these specific indices, set 'Ngày' to NaT to make them eligible for bfill
    final_combined_df.loc[mismatch_indices_bfill, "Ngày"] = pd.NaT
    # Apply backward fill to the entire 'Ngày' column
    final_combined_df["Ngày"] = final_combined_df["Ngày"].bfill()
    print(
        f"Handled {len(mismatch_indices_bfill)} conflicting date entries with backward fill."
    )
else:
    print("No conflicting date entries found for backward fill.")

# Drop the temporary column used for bfill verification
final_combined_df = final_combined_df.drop(
    columns=["Verified_Ngày_temp_bfill"], errors="ignore"
)

# --- 4. Resolve ALL identified mismatches by enforcing source year/month ---
final_combined_df["Verified_Ngày_temp_enforce"] = pd.to_datetime(
    final_combined_df["Ngày"], errors="coerce"
)

mismatch_mask_enforce = final_combined_df["Verified_Ngày_temp_enforce"].notna() & (
    (
        final_combined_df["Verified_Ngày_temp_enforce"].dt.year
        != final_combined_df["_source_file_year"]
    )
    | (
        final_combined_df["Verified_Ngày_temp_enforce"].dt.month
        != final_combined_df["_source_file_month"]
    )
)
mismatched_all_rows_enforce = final_combined_df[mismatch_mask_enforce]

if not mismatched_all_rows_enforce.empty:
    fixed_dates_all = pd.Series(
        pd.NaT, index=mismatched_all_rows_enforce.index, dtype="datetime64[ns]"
    )
    for idx, row in mismatched_all_rows_enforce.iterrows():
        try:
            current_date_obj = pd.to_datetime(
                final_combined_df.loc[idx, "Ngày"], errors="coerce"
            )
            original_day = current_date_obj.day if pd.notna(current_date_obj) else 1
            # Ensure day is valid for the target month/year
            max_days = pd.Timestamp(
                int(row["_source_file_year"]), int(row["_source_file_month"]), 1
            ).days_in_month
            day_to_use = min(original_day, max_days)

            new_date = datetime(
                int(row["_source_file_year"]),
                int(row["_source_file_month"]),
                day_to_use,
            )
        except ValueError:
            new_date = datetime(
                int(row["_source_file_year"]), int(row["_source_file_month"]), 1
            )
        fixed_dates_all.loc[idx] = new_date
    final_combined_df.loc[fixed_dates_all.index, "Ngày"] = fixed_dates_all
    print(
        f"Resolved {len(fixed_dates_all)} year/month mismatches by enforcing source year/month."
    )
else:
    print("No year/month mismatches found to resolve by enforcing source year/month.")

final_combined_df = final_combined_df.drop(
    columns=["Verified_Ngày_temp_enforce"], errors="ignore"
)

# --- 5. Format 'Ngày' as ISO date string (YYYY-MM-DD) ---
final_combined_df["Ngày"] = (
    final_combined_df["Ngày"].dt.strftime("%Y-%m-%d").fillna("")
)

# --- Save to CSV ---
output_dir = os.path.join(os.getcwd(), "data", "final")
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# --- 6. Rename source file year/month columns ---
final_combined_df = final_combined_df.rename(
    columns={"_source_file_month": "Tháng", "_source_file_year": "Năm"}
)

# --- 6a. Reorder columns: common columns first, then different ones ---
column_order = [
    "Mã hàng",
    "Tên hàng",
    "Số lượng",
    "Đơn giá",
    "Thành tiền",
    "Ngày",
    "Tháng",
    "Năm",
    "Mã chứng từ",
    "Tên nhà cung cấp",  # Different column
]
final_combined_df = final_combined_df[[col for col in column_order if col in final_combined_df.columns]]

# Create a temporary datetime column for the source file date
# using 'Năm' and 'Tháng' after renaming
final_combined_df["temp_source_file_date"] = pd.to_datetime(
    final_combined_df["Năm"].astype(str)
    + "-"
    + final_combined_df["Tháng"].astype(str)
    + "-01",
    errors="coerce",  # Handle potential conversion errors
)

# Get the minimum and maximum source file dates
min_source_date = final_combined_df["temp_source_file_date"].min()
max_source_date = final_combined_df["temp_source_file_date"].max()

# Extract year and month from these min/max dates
min_year_out = min_source_date.year
min_month_out = min_source_date.month
max_year_out = max_source_date.year
max_month_out = max_source_date.month

output_filename = f"{min_year_out:04d}_{min_month_out:02d}_{max_year_out:04d}_{max_month_out:02d}_CT.NHAP_processed.csv"
output_filepath = os.path.join(output_dir, output_filename)

# Drop the temporary column before saving
final_combined_df = final_combined_df.drop(columns=["temp_source_file_date"])

# --- Format columns before saving ---
# Ensure text columns are strings, numeric columns are numeric
if "Mã hàng" in final_combined_df.columns:
    final_combined_df["Mã hàng"] = final_combined_df["Mã hàng"].astype(str)
if "Mã chứng từ" in final_combined_df.columns:
    final_combined_df["Mã chứng từ"] = final_combined_df["Mã chứng từ"].astype(str)
if "Tên nhà cung cấp" in final_combined_df.columns:
    final_combined_df["Tên nhà cung cấp"] = final_combined_df["Tên nhà cung cấp"].astype(str)
if "Tên hàng" in final_combined_df.columns:
    final_combined_df["Tên hàng"] = final_combined_df["Tên hàng"].astype(str)

# Numeric columns
for col in ["Số lượng", "Đơn giá", "Thành tiền"]:
    if col in final_combined_df.columns:
        final_combined_df[col] = pd.to_numeric(final_combined_df[col], errors="coerce")

# Integer columns
for col in ["Tháng", "Năm"]:
    if col in final_combined_df.columns:
        final_combined_df[col] = pd.to_numeric(final_combined_df[col], errors="coerce").astype("Int64")

# --- Sort by Ngày and Mã chứng từ ---
final_combined_df["Ngày_dt"] = pd.to_datetime(final_combined_df["Ngày"], errors="coerce")
final_combined_df = final_combined_df.sort_values(by=["Ngày_dt", "Mã chứng từ"], na_position="last")
final_combined_df = final_combined_df.drop(columns=["Ngày_dt"])

final_combined_df.to_csv(output_filepath, index=False, encoding="utf-8")
print(f"\nFinal combined DataFrame saved to: {output_filepath}")

# --- Final Display of the cleaned DataFrame ---
print("\n--- Final Combined DataFrame after all processing and cleaning ---")
# display(final_combined_df.head())
# display(final_combined_df.info(memory_usage='deep'))
